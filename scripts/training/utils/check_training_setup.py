#!/usr/bin/env python3
"""
Script ki·ªÉm tra c·∫•u h√¨nh training v√† hi·ªáu su·∫•t
S·ª≠ d·ª•ng script n√†y ƒë·ªÉ t√¨m nguy√™n nh√¢n training ch·∫≠m tr√™n local
"""

import torch
import subprocess
from pathlib import Path

def print_header(text):
    print(f"\n{'='*60}")
    print(f"  {text}")
    print(f"{'='*60}\n")

def print_info(text):
    print(f"‚ÑπÔ∏è  {text}")

def print_success(text):
    print(f"‚úÖ {text}")

def print_warning(text):
    print(f"‚ö†Ô∏è  {text}")

def print_error(text):
    print(f"‚ùå {text}")

def check_nvidia_smi():
    """Ki·ªÉm tra nvidia-smi"""
    try:
        result = subprocess.run(
            ["nvidia-smi", "--query-gpu=name,driver_version,cuda_version", "--format=csv,noheader"],
            capture_output=True,
            text=True,
            timeout=5
        )
        if result.returncode == 0:
            return result.stdout.strip()
        return None
    except (subprocess.TimeoutExpired, FileNotFoundError, subprocess.SubprocessError):
        return None

def check_gpu():
    """Ki·ªÉm tra GPU v√† CUDA"""
    print_header("KI·ªÇM TRA GPU")
    
    # Ki·ªÉm tra nvidia-smi tr∆∞·ªõc
    nvidia_info = check_nvidia_smi()
    if nvidia_info:
        print_success("T√¨m th·∫•y GPU qua nvidia-smi:")
        for line in nvidia_info.split('\n'):
            if line.strip():
                parts = line.split(', ')
                if len(parts) >= 3:
                    gpu_name = parts[0].strip()
                    driver_version = parts[1].strip()
                    cuda_version_system = parts[2].strip()
                    print_info(f"  GPU: {gpu_name}")
                    print_info(f"  Driver: {driver_version}")
                    print_info(f"  CUDA (System): {cuda_version_system}")
        print()
    
    if not torch.cuda.is_available():
        print_error("‚ùå PyTorch KH√îNG nh·∫≠n di·ªán ƒë∆∞·ª£c GPU!")
        print()
        
        if nvidia_info:
            print_warning("‚ö†Ô∏è V·∫§N ƒê·ªÄ PH√ÅT HI·ªÜN:")
            print_info("  - H·ªá th·ªëng c√≥ GPU v√† CUDA driver")
            print_info(f"  - PyTorch version: {torch.__version__}")
            pytorch_cuda = torch.version.cuda if hasattr(torch.version, 'cuda') else "N/A"
            if pytorch_cuda and pytorch_cuda != "N/A":
                print_info(f"  - PyTorch CUDA: {pytorch_cuda}")
            print()
            print_error("NGUY√äN NH√ÇN:")
            print_info("  PyTorch ƒë∆∞·ª£c compile v·ªõi CUDA version kh√°c v·ªõi CUDA tr√™n h·ªá th·ªëng")
            print_info("  ‚Üí PyTorch kh√¥ng th·ªÉ s·ª≠ d·ª•ng GPU")
            print()
            print_success("GI·∫¢I PH√ÅP:")
            print_info("1. G·ª° PyTorch hi·ªán t·∫°i:")
            print_info("   pip uninstall torch torchvision torchaudio")
            print()
            print_info("2. C√†i ƒë·∫∑t PyTorch v·ªõi CUDA 12.1 (t∆∞∆°ng th√≠ch v·ªõi CUDA 13.0):")
            print_info("   pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121")
            print()
            print_info("3. Ho·∫∑c c√†i ƒë·∫∑t PyTorch v·ªõi CUDA 12.4:")
            print_info("   pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124")
            print()
            print_info("4. Ki·ªÉm tra l·∫°i:")
            print_info("   python -c \"import torch; print('CUDA available:', torch.cuda.is_available())\"")
            print()
            print_info("5. N·∫øu v·∫´n kh√¥ng ƒë∆∞·ª£c, ki·ªÉm tra CUDA toolkit:")
            print_info("   - ƒê·∫£m b·∫£o CUDA toolkit ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t")
            print_info("   - Ki·ªÉm tra PATH c√≥ ch·ª©a CUDA bin kh√¥ng")
            print_info("   - Th·ª≠ restart terminal/IDE")
        else:
            print_warning("GPU kh√¥ng kh·∫£ d·ª•ng - Training s·∫Ω r·∫•t ch·∫≠m tr√™n CPU")
            print_info("Nguy√™n nh√¢n c√≥ th·ªÉ:")
            print_info("  1. Ch∆∞a c√†i ƒë·∫∑t CUDA driver")
            print_info("  2. PyTorch kh√¥ng ƒë∆∞·ª£c compile v·ªõi CUDA support")
            print_info("  3. GPU kh√¥ng ƒë∆∞·ª£c nh·∫≠n di·ªán b·ªüi driver")
            print()
            print_info("Gi·∫£i ph√°p:")
            print_info("  - C√†i ƒë·∫∑t CUDA driver: https://developer.nvidia.com/cuda-downloads")
            print_info("  - C√†i ƒë·∫∑t PyTorch v·ªõi CUDA: https://pytorch.org/get-started/locally/")
            print_info("  - Ho·∫∑c train tr√™n Google Colab (c√≥ GPU mi·ªÖn ph√≠)")
        
        return False
    else:
        gpu_name = torch.cuda.get_device_name(0)
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
        cuda_version = torch.version.cuda
        print_success(f"‚úÖ GPU: {gpu_name}")
        print_success(f"‚úÖ GPU Memory: {gpu_memory:.2f} GB")
        print_success(f"‚úÖ CUDA Version (PyTorch): {cuda_version}")
        if nvidia_info:
            print_info("‚úÖ GPU ƒë√£ ƒë∆∞·ª£c PyTorch nh·∫≠n di·ªán th√†nh c√¥ng!")
        print()
        print_info("GPU ƒë√£ s·∫µn s√†ng - Training s·∫Ω nhanh h∆°n nhi·ªÅu!")
        return True

def check_pytorch():
    """Ki·ªÉm tra PyTorch version"""
    print_header("KI·ªÇM TRA PYTORCH")
    
    print_info(f"PyTorch Version: {torch.__version__}")
    
    if torch.cuda.is_available():
        print_info("CUDA Available: ‚úÖ")
        print_info(f"cuDNN Version: {torch.backends.cudnn.version()}")
    else:
        print_info("CUDA Available: ‚ùå")
    
    return True

def check_config():
    """Ki·ªÉm tra config.yml"""
    print_header("KI·ªÇM TRA CONFIG")
    
    config_path = Path("config.yml")
    if not config_path.exists():
        print_error("Kh√¥ng t√¨m th·∫•y config.yml")
        return False
    
    print_success("T√¨m th·∫•y config.yml")
    
    # ƒê·ªçc config v√† ki·ªÉm tra c√°c th√¥ng s·ªë quan tr·ªçng
    with open(config_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # Ki·ªÉm tra epochs
    if 'epochs: 600' in content:
        print_warning("Epochs: 600 (r·∫•t cao - training s·∫Ω m·∫•t nhi·ªÅu th·ªùi gian)")
        print_info("Khuy·∫øn ngh·ªã: Gi·∫£m xu·ªëng 300-400 cho training local")
    
    # Ki·ªÉm tra batch_size
    if 'batch_size: [16, 32]' in content:
        has_gpu = torch.cuda.is_available()
        if has_gpu:
            print_info("Batch size: [16, 32] - Ph√π h·ª£p v·ªõi GPU")
        else:
            print_warning("Batch size: [16, 32] - C√≥ th·ªÉ qu√° l·ªõn cho CPU")
            print_info("Khuy·∫øn ngh·ªã: Gi·∫£m xu·ªëng [8, 16] khi train tr√™n CPU")
    
    # Ki·ªÉm tra PhoBERT batch_size
    if 'batch_size: 32' in content or 'batch_size:' in content:
        print_info("PhoBERTFeaturizer batch_size ƒë√£ ƒë∆∞·ª£c c·∫•u h√¨nh")
    else:
        print_warning("PhoBERTFeaturizer ch∆∞a c√≥ batch_size - s·∫Ω d√πng m·∫∑c ƒë·ªãnh")
        print_info("Khuy·∫øn ngh·ªã: Th√™m batch_size: 32 trong config.yml")
    
    return True

def check_model_files():
    """Ki·ªÉm tra model files"""
    print_header("KI·ªÇM TRA MODEL FILES")
    
    model_paths = [
        Path("models/phobert-large"),
        Path("models_hub/phobert-large"),
    ]
    
    found = False
    for model_path in model_paths:
        if model_path.exists():
            print_success(f"T√¨m th·∫•y model t·∫°i: {model_path}")
            # Ki·ªÉm tra c√°c file quan tr·ªçng
            required_files = ["config.json", "pytorch_model.bin", "vocab.txt"]
            missing = []
            for file in required_files:
                file_path = model_path / file
                if file_path.exists():
                    print_info("  ‚úÖ {}".format(file))
                else:
                    print_warning("  ‚ùå {} - thi·∫øu".format(file))
                    missing.append(file)
            
            if not missing:
                found = True
                break
            else:
                print_warning("Model t·∫°i {} ch∆∞a ƒë·∫ßy ƒë·ªß".format(model_path))
    
    if not found:
        print_warning("Kh√¥ng t√¨m th·∫•y PhoBERT model ƒë·∫ßy ƒë·ªß")
        print_info("Model s·∫Ω ƒë∆∞·ª£c t·∫£i t·ª± ƒë·ªông khi training, nh∆∞ng s·∫Ω m·∫•t th·ªùi gian")
    
    return found

def check_training_data():
    """Ki·ªÉm tra training data"""
    print_header("KI·ªÇM TRA TRAINING DATA")
    
    data_path = Path("data/nlu.yml")
    if not data_path.exists():
        print_error("Kh√¥ng t√¨m th·∫•y data/nlu.yml")
        return False
    
    print_success("T√¨m th·∫•y data/nlu.yml")
    
    # ƒê·∫øm s·ªë l∆∞·ª£ng examples (∆∞·ªõc t√≠nh)
    with open(data_path, 'r', encoding='utf-8') as f:
        content = f.read()
        # ƒê·∫øm s·ªë d√≤ng c√≥ "- " (examples)
        examples_count = content.count('\n      - ')
    
    print_info(f"∆Ø·ªõc t√≠nh s·ªë examples: ~{examples_count}")
    
    if examples_count > 1000:
        print_info("Dataset l·ªõn - training s·∫Ω m·∫•t nhi·ªÅu th·ªùi gian h∆°n")
        print_info("Khuy·∫øn ngh·ªã: S·ª≠ d·ª•ng GPU ƒë·ªÉ tƒÉng t·ªëc")
    
    return True

def check_memory():
    """Ki·ªÉm tra RAM"""
    print_header("KI·ªÇM TRA MEMORY")
    
    try:
        import psutil
        memory = psutil.virtual_memory()
        total_gb = memory.total / (1024**3)
        available_gb = memory.available / (1024**3)
        
        print_info(f"Total RAM: {total_gb:.2f} GB")
        print_info(f"Available RAM: {available_gb:.2f} GB")
        
        if total_gb < 8:
            print_warning("RAM < 8GB - C√≥ th·ªÉ g·∫∑p v·∫•n ƒë·ªÅ Out of Memory")
            print_info("Khuy·∫øn ngh·ªã: Gi·∫£m batch_size trong config.yml")
        elif total_gb < 16:
            print_info("RAM 8-16GB - ƒê·ªß cho training nh∆∞ng n√™n c·∫©n th·∫≠n v·ªõi batch_size")
        else:
            print_success("RAM >= 16GB - ƒê·ªß cho training")
    except ImportError:
        print_warning("Kh√¥ng th·ªÉ ki·ªÉm tra RAM (c·∫ßn c√†i psutil)")
        print_info("Ch·∫°y: pip install psutil")

def get_recommendations():
    """ƒê∆∞a ra c√°c khuy·∫øn ngh·ªã t·ªëi ∆∞u"""
    print_header("KHUY·∫æN NGH·ªä T·ªêI ∆ØU")
    
    has_gpu = torch.cuda.is_available()
    
    if not has_gpu:
        print_warning("‚ö†Ô∏è  TRAINING TR√äN CPU S·∫º R·∫§T CH·∫¨M")
        print()
        print_info("C√°c gi·∫£i ph√°p:")
        print_info("1. Train tr√™n Google Colab (c√≥ GPU mi·ªÖn ph√≠)")
        print_info("2. C√†i ƒë·∫∑t CUDA v√† PyTorch v·ªõi GPU support")
        print_info("3. Gi·∫£m epochs xu·ªëng 200-300")
        print_info("4. Gi·∫£m batch_size xu·ªëng [8, 16]")
        print_info("5. S·ª≠ d·ª•ng PhoBERT-base thay v√¨ Large")
    else:
        print_success("GPU ƒë√£ s·∫µn s√†ng - Training s·∫Ω nhanh h∆°n!")
        print()
        print_info("C√°c t·ªëi ∆∞u h√≥a:")
        print_info("1. Gi·ªØ batch_size: [16, 32] ho·∫∑c tƒÉng l√™n [32, 64]")
        print_info("2. TƒÉng PhoBERTFeaturizer batch_size l√™n 64-128")
        print_info("3. C√≥ th·ªÉ train v·ªõi 600 epochs (m·∫•t ~20-40 ph√∫t tr√™n GPU)")
    
    print()
    print_info("C√°c c·∫£i ti·∫øn ƒë√£ ƒë∆∞·ª£c th√™m v√†o code:")
    print_info("  ‚úÖ Batch processing trong PhoBERTFeaturizer (nhanh h∆°n 10-50x)")
    print_info("  ‚úÖ T·ª± ƒë·ªông detect GPU v√† hi·ªÉn th·ªã th√¥ng tin")
    print_info("  ‚úÖ C·∫•u h√¨nh batch_size cho featurizer")

def main():
    print("üîç KI·ªÇM TRA C·∫§U H√åNH TRAINING")
    print("=" * 60)
    
    # Ki·ªÉm tra c√°c th√†nh ph·∫ßn
    check_pytorch()
    has_gpu = check_gpu()
    check_config()
    check_model_files()
    check_training_data()
    check_memory()
    get_recommendations()
    
    print_header("K·∫æT LU·∫¨N")
    
    if not has_gpu:
        print_warning("Training tr√™n CPU s·∫Ω m·∫•t 2-5 gi·ªù (ho·∫∑c h∆°n)")
        print_info("Khuy·∫øn ngh·ªã: Train tr√™n Google Colab ho·∫∑c c√†i ƒë·∫∑t GPU")
    else:
        print_success("C·∫•u h√¨nh t·ªët - Training s·∫Ω nhanh h∆°n nhi·ªÅu!")
        print_info("V·ªõi GPU, training m·∫•t kho·∫£ng 20-40 ph√∫t")
    
    print()
    print_info("Sau khi t·ªëi ∆∞u h√≥a batch processing, training s·∫Ω nhanh h∆°n 10-50 l·∫ßn!")
    print_info("Ch·∫°y: rasa train nlu ƒë·ªÉ b·∫Øt ƒë·∫ßu training")

if __name__ == "__main__":
    main()

