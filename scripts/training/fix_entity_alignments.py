#!/usr/bin/env python3
"""
Script ƒë·ªÉ fix entity alignment trong training data
ƒêi·ªÅu ch·ªânh entity annotations ƒë·ªÉ kh·ªõp v·ªõi token boundaries c·ªßa WhitespaceTokenizer

V·∫•n ƒë·ªÅ: Entity annotations trong format [value](type) c√≥ th·ªÉ kh√¥ng kh·ªõp v·ªõi token boundaries
sau khi tokenize, g√¢y ra warnings "Misaligned entity annotation"

Gi·∫£i ph√°p: 
1. S·ª≠ d·ª•ng Rasa's tokenizer ƒë·ªÉ tokenize text
2. ƒêi·ªÅu ch·ªânh entity annotations ƒë·ªÉ ch·ªâ bao g·ªìm c√°c tokens ho√†n ch·ªânh
3. Lo·∫°i b·ªè c√°c k√Ω t·ª± th·ª´a (whitespace, punctuation) kh·ªèi entity boundaries
"""

import re
import yaml
from pathlib import Path
from typing import List, Tuple, Dict, Any, Optional
import sys


def tokenize_whitespace(text: str) -> List[Tuple[int, int, str]]:
    """
    Tokenize text theo whitespace (gi·ªëng WhitespaceTokenizer c·ªßa Rasa)
    Tr·∫£ v·ªÅ list (start, end, token)
    """
    tokens = []
    words = text.split()
    current_pos = 0
    
    for word in words:
        # T√¨m v·ªã tr√≠ c·ªßa word trong text (t√≠nh t·ª´ current_pos ƒë·ªÉ tr√°nh tr√πng)
        start = text.find(word, current_pos)
        if start == -1:
            # Fallback: skip whitespace t·ª´ current_pos
            start = current_pos
            while start < len(text) and text[start].isspace():
                start += 1
            if start >= len(text):
                break
        
        end = start + len(word)
        tokens.append((start, end, word))
        current_pos = end
    
    return tokens


def strip_punctuation(text: str) -> str:
    """Lo·∫°i b·ªè punctuation ·ªü ƒë·∫ßu v√† cu·ªëi text"""
    import string
    # Vietnamese punctuation + English punctuation
    punctuation = string.punctuation + '.,;:!?„ÄÇÔºåÔºõÔºöÔºÅÔºü'
    return text.strip(punctuation)


def find_best_token_alignment(
    text: str,
    entity_value: str,
    tokens: List[Tuple[int, int, str]]
) -> Optional[Tuple[int, int, str]]:
    """
    T√¨m alignment t·ªët nh·∫•t cho entity value v·ªõi tokens
    Tr·∫£ v·ªÅ (start, end, aligned_value) ho·∫∑c None
    
    Lo·∫°i b·ªè punctuation t·ª´ entity boundaries ƒë·ªÉ tr√°nh misalignment
    """
    # Normalize: lo·∫°i b·ªè spaces v√† punctuation th·ª´a
    entity_value_clean = strip_punctuation(entity_value.strip())
    entity_words = entity_value_clean.split()
    
    if not entity_words:
        return None
    
    # Case 1: Single word entity
    if len(entity_words) == 1:
        entity_word = entity_words[0]
        for token_start, token_end, token_text in tokens:
            token_clean = strip_punctuation(token_text)
            # So s√°nh kh√¥ng ph√¢n bi·ªát hoa th∆∞·ªùng, lo·∫°i b·ªè punctuation
            if token_clean.lower() == entity_word.lower():
                # Tr·∫£ v·ªÅ token kh√¥ng c√≥ punctuation
                return (token_start, token_end, token_clean)
        # Kh√¥ng t√¨m th·∫•y exact match, th·ª≠ t√¨m partial match (kh√¥ng c√≥ punctuation)
        for token_start, token_end, token_text in tokens:
            token_clean = strip_punctuation(token_text)
            if entity_word.lower() in token_clean.lower() or token_clean.lower() in entity_word.lower():
                return (token_start, token_end, token_clean)
    
    # Case 2: Multi-word entity
    # T√¨m sequence of tokens kh·ªõp v·ªõi entity words (lo·∫°i b·ªè punctuation)
    token_texts_clean = [strip_punctuation(t[2]) for t in tokens]
    
    # T√¨m v·ªã tr√≠ b·∫Øt ƒë·∫ßu c·ªßa sequence
    for i in range(len(token_texts_clean) - len(entity_words) + 1):
        # Ki·ªÉm tra xem sequence t·ª´ i c√≥ kh·ªõp kh√¥ng
        match = True
        matching_tokens = []
        for j, entity_word in enumerate(entity_words):
            if i + j >= len(token_texts_clean):
                match = False
                break
            token_clean = token_texts_clean[i + j]
            # So s√°nh kh√¥ng ph√¢n bi·ªát hoa th∆∞·ªùng
            if token_clean.lower() != entity_word.lower():
                match = False
                break
            matching_tokens.append(tokens[i + j])
        
        if match and matching_tokens:
            # T√¨m th·∫•y sequence kh·ªõp
            start_token = matching_tokens[0]
            end_token = matching_tokens[-1]
            aligned_start = start_token[0]
            aligned_end = end_token[1]
            # Extract value t·ª´ text, lo·∫°i b·ªè punctuation ·ªü boundaries
            aligned_value_raw = text[aligned_start:aligned_end]
            aligned_value = ' '.join([strip_punctuation(t[2]) for t in matching_tokens])
            return (aligned_start, aligned_end, aligned_value)
    
    # Case 3: T√¨m trong text (c√≥ th·ªÉ c√≥ format kh√°c)
    # T√¨m v·ªã tr√≠ c·ªßa entity_value trong text (case insensitive, lo·∫°i b·ªè punctuation)
    text_lower = text.lower()
    entity_lower = entity_value_clean.lower()
    
    # T√¨m c√°c v·ªã tr√≠ c√≥ th·ªÉ kh·ªõp
    pos = text_lower.find(entity_lower)
    if pos != -1:
        # T√¨m tokens n·∫±m trong kho·∫£ng n√†y
        entity_start = pos
        entity_end = pos + len(entity_value_clean)
        
        overlapping_tokens = []
        for token_start, token_end, token_text in tokens:
            # Token overlaps v·ªõi entity (t√≠nh c·∫£ punctuation)
            if token_start < entity_end + 2 and token_end > entity_start - 2:
                overlapping_tokens.append((token_start, token_end, token_text))
        
        if overlapping_tokens:
            # L·ªçc tokens ƒë·ªÉ ch·ªâ l·∫•y nh·ªØng token c√≥ text kh·ªõp
            matching_tokens = []
            for token_start, token_end, token_text in overlapping_tokens:
                token_clean = strip_punctuation(token_text).lower()
                if any(token_clean == ew.lower() for ew in entity_words):
                    matching_tokens.append((token_start, token_end, token_text))
            
            if matching_tokens:
                aligned_start = min(t[0] for t in matching_tokens)
                aligned_end = max(t[1] for t in matching_tokens)
                aligned_value = ' '.join([strip_punctuation(t[2]) for t in matching_tokens])
                return (aligned_start, aligned_end, aligned_value)
    
    return None


def fix_entity_in_example(example: str) -> str:
    """
    Fix entity annotations trong m·ªôt example
    Format: "text with [entity](type)"
    """
    # Parse entities t·ª´ format [value](type)
    entity_pattern = r'\[([^\]]+)\]\(([^)]+)\)'
    entities = re.findall(entity_pattern, example)
    
    if not entities:
        return example  # Kh√¥ng c√≥ entities
    
    # L·∫•y text g·ªëc (thay th·∫ø [entity](type) b·∫±ng entity value)
    text_only = re.sub(entity_pattern, r'\1', example)
    
    # Tokenize text
    tokens = tokenize_whitespace(text_only)
    
    if not tokens:
        return example
    
    # Fix t·ª´ng entity (x·ª≠ l√Ω t·ª´ cu·ªëi l√™n ƒë·ªÉ tr√°nh offset issues)
    fixed_example = example
    
    for entity_value, entity_type in reversed(entities):
        # Clean entity value (lo·∫°i b·ªè punctuation)
        entity_clean = strip_punctuation(entity_value.strip())
        entity_words = entity_clean.split()
        
        if not entity_words:
            continue
        
        # T√¨m entity trong tokens (kh√¥ng c√≥ punctuation)
        token_texts_clean = [strip_punctuation(t[2]) for t in tokens]
        
        # T√¨m sequence kh·ªõp
        found_match = False
        for i in range(len(token_texts_clean) - len(entity_words) + 1):
            # Ki·ªÉm tra sequence
            match = True
            matching_tokens = []
            for j, ew in enumerate(entity_words):
                if i + j >= len(token_texts_clean):
                    match = False
                    break
                if token_texts_clean[i + j].lower() != ew.lower():
                    match = False
                    break
                matching_tokens.append(tokens[i + j])
            
            if match and matching_tokens:
                # T√¨m th·∫•y match - t·∫°o aligned value t·ª´ tokens (kh√¥ng c√≥ punctuation)
                aligned_value = ' '.join([strip_punctuation(t[2]) for t in matching_tokens])
                
                # Ch·ªâ fix n·∫øu gi√° tr·ªã kh√°c (kh√¥ng ph√¢n bi·ªát hoa th∆∞·ªùng)
                if aligned_value.strip().lower() != entity_clean.strip().lower():
                    old_annotation = f'[{entity_value}]({entity_type})'
                    new_annotation = f'[{aligned_value}]({entity_type})'
                    fixed_example = fixed_example.replace(old_annotation, new_annotation, 1)
                    found_match = True
                    break
        
        # N·∫øu kh√¥ng t√¨m th·∫•y match, th·ª≠ v·ªõi c√°c aliases ph·ªï bi·∫øn
        if not found_match:
            # Map c√°c aliases
            entity_aliases_map = {
                'TP.HCM': ['th√†nh ph·ªë hcm', 'hcm'],
                'TP HCM': ['th√†nh ph·ªë hcm', 'hcm'],
                'HCM': ['hcm', 'th√†nh ph·ªë hcm'],
                'Sai Gon': ['s√†i g√≤n', 'sai gon'],
            }
            
            if entity_value in entity_aliases_map:
                for alias in entity_aliases_map[entity_value]:
                    alias_words = alias.split()
                    for i in range(len(token_texts_clean) - len(alias_words) + 1):
                        match = True
                        matching_tokens = []
                        for j, aw in enumerate(alias_words):
                            if i + j >= len(token_texts_clean):
                                match = False
                                break
                            if token_texts_clean[i + j].lower() != aw.lower():
                                match = False
                                break
                            matching_tokens.append(tokens[i + j])
                        
                        if match and matching_tokens:
                            aligned_value = ' '.join([strip_punctuation(t[2]) for t in matching_tokens])
                            old_annotation = f'[{entity_value}]({entity_type})'
                            new_annotation = f'[{aligned_value}]({entity_type})'
                            fixed_example = fixed_example.replace(old_annotation, new_annotation, 1)
                            found_match = True
                            break
                    
                    if found_match:
                        break
    
    return fixed_example


def fix_nlu_file(input_file: Path, output_file: Path = None):
    """
    Fix entity alignments trong file nlu.yml
    
    C√°ch ti·∫øp c·∫≠n:
    1. ƒê·ªçc file YAML
    2. V·ªõi m·ªói example c√≥ entity annotations, tokenize text
    3. ƒêi·ªÅu ch·ªânh entity annotations ƒë·ªÉ kh·ªõp v·ªõi token boundaries
    4. Ghi l·∫°i file
    """
    if output_file is None:
        output_file = input_file
    
    print(f"üìñ ƒê·ªçc file: {input_file}")
    
    # Backup file g·ªëc
    backup_file = input_file.with_suffix('.yml.bak')
    if not backup_file.exists():
        print(f"üíæ Backup file g·ªëc: {backup_file}")
        import shutil
        shutil.copy2(input_file, backup_file)
    
    # ƒê·ªçc file nh∆∞ text ƒë·ªÉ gi·ªØ nguy√™n format
    with open(input_file, 'r', encoding='utf-8') as f:
        lines = f.readlines()
    
    print(f"üîç ƒêang x·ª≠ l√Ω {len(lines)} d√≤ng...")
    
    fixed_lines = []
    fixed_count = 0
    total_examples = 0
    
    for line_num, line in enumerate(lines, 1):
        original_line = line
        
        # Ch·ªâ x·ª≠ l√Ω c√°c d√≤ng c√≥ entity annotations (b·∫Øt ƒë·∫ßu b·∫±ng '- ' v√† c√≥ '[entity](type)')
        if line.strip().startswith('- ') and '[' in line and '](' in line:
            # Extract example (b·ªè '- ' ·ªü ƒë·∫ßu)
            example = line[2:].strip()
            total_examples += 1
            
            # Fix entity annotations
            fixed_example = fix_entity_in_example(example)
            
            if fixed_example != example:
                fixed_count += 1
                if fixed_count <= 10:  # Ch·ªâ hi·ªÉn th·ªã 10 examples ƒë·∫ßu ti√™n
                    print(f"   ‚úÖ Line {line_num}: Fixed")
                    print(f"      Old: {example[:60]}...")
                    print(f"      New: {fixed_example[:60]}...")
                
                fixed_lines.append(f"      - {fixed_example}\n")
            else:
                fixed_lines.append(line)
        else:
            fixed_lines.append(line)
    
    print(f"\nüìä Th·ªëng k√™:")
    print(f"   - T·ªïng s·ªë examples c√≥ entities: {total_examples}")
    print(f"   - ƒê√£ fix: {fixed_count}")
    print(f"   - Kh√¥ng c·∫ßn fix: {total_examples - fixed_count}")
    
    # Write to output file
    print(f"\nüíæ Ghi file: {output_file}")
    with open(output_file, 'w', encoding='utf-8') as f:
        f.writelines(fixed_lines)
    
    print("‚úÖ Ho√†n t·∫•t!")
    print(f"üí° File backup ƒë∆∞·ª£c l∆∞u t·∫°i: {backup_file}")
    print(f"üí° B·∫°n c√≥ th·ªÉ so s√°nh ƒë·ªÉ xem c√°c thay ƒë·ªïi")


def main():
    """Main function"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Fix entity alignments in NLU training data')
    parser.add_argument(
        'input_file',
        type=str,
        help='Input NLU YAML file (e.g., data/nlu.yml)'
    )
    parser.add_argument(
        '-o', '--output',
        type=str,
        default=None,
        help='Output file (default: overwrite input file)'
    )
    
    args = parser.parse_args()
    
    input_path = Path(args.input_file)
    if not input_path.exists():
        print(f"‚ùå File kh√¥ng t·ªìn t·∫°i: {input_path}")
        sys.exit(1)
    
    output_path = Path(args.output) if args.output else input_path
    
    fix_nlu_file(input_path, output_path)


if __name__ == '__main__':
    main()

