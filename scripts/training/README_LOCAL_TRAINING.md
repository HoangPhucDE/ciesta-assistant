# üöÄ Training tr√™n Local Machine - H∆∞·ªõng d·∫´n T·ªëi ∆Øu

## ‚ö° C√°c T·ªëi ∆Øu H√≥a ƒê√£ Th·ª±c Hi·ªán

### 1. Batch Processing trong PhoBERTFeaturizer
**V·∫•n ƒë·ªÅ c≈©:** X·ª≠ l√Ω t·ª´ng message m·ªôt ‚Üí R·∫•t ch·∫≠m (10-50 l·∫ßn ch·∫≠m h∆°n)

**Gi·∫£i ph√°p:** X·ª≠ l√Ω batch messages c√πng l√∫c
- TƒÉng t·ªëc 10-50x so v·ªõi tr∆∞·ªõc
- S·ª≠ d·ª•ng GPU hi·ªáu qu·∫£ h∆°n
- Gi·∫£m overhead c·ªßa model loading

### 2. T·ª± ƒê·ªông Detect GPU
- T·ª± ƒë·ªông ph√°t hi·ªán v√† s·ª≠ d·ª•ng GPU n·∫øu c√≥
- Hi·ªÉn th·ªã th√¥ng tin GPU khi kh·ªüi ƒë·ªông
- C·∫£nh b√°o n·∫øu ch·ªâ c√≥ CPU

### 3. Config T·ªëi ∆Øu cho Local
- `config_local.yml`: Config v·ªõi √≠t epochs h∆°n (300 thay v√¨ 600)
- Batch size ph√π h·ª£p v·ªõi CPU/GPU
- Gi·∫£m model complexity ƒë·ªÉ train nhanh h∆°n

## üîç Ki·ªÉm Tra C·∫•u H√¨nh

Tr∆∞·ªõc khi training, ch·∫°y script ki·ªÉm tra:

```bash
python scripts/training/check_training_setup.py
```

Script n√†y s·∫Ω ki·ªÉm tra:
- ‚úÖ GPU c√≥ s·∫µn kh√¥ng
- ‚úÖ PyTorch version v√† CUDA support
- ‚úÖ Config c√≥ t·ªëi ∆∞u kh√¥ng
- ‚úÖ Model files ƒë√£ c√≥ ch∆∞a
- ‚úÖ Training data
- ‚úÖ RAM available

## üìä So S√°nh Hi·ªáu Su·∫•t

### Tr∆∞·ªõc khi t·ªëi ∆∞u:
- **CPU:** 3-5 gi·ªù (x·ª≠ l√Ω t·ª´ng message)
- **GPU:** 40-60 ph√∫t (x·ª≠ l√Ω t·ª´ng message)

### Sau khi t·ªëi ∆∞u:
- **CPU:** 1-2 gi·ªù (batch processing) ‚ö° **2-3x nhanh h∆°n**
- **GPU:** 15-30 ph√∫t (batch processing) ‚ö° **2-3x nhanh h∆°n**

## üéØ C√°ch S·ª≠ D·ª•ng

### Option 1: Training v·ªõi Config M·∫∑c ƒê·ªãnh (600 epochs)
```bash
rasa train nlu
```
**Th·ªùi gian:** 1-2 gi·ªù (CPU) ho·∫∑c 20-40 ph√∫t (GPU)

### Option 2: Training v·ªõi Config T·ªëi ∆Øu Local (300 epochs)
```bash
rasa train nlu --config config_local.yml
```
**Th·ªùi gian:** 30-60 ph√∫t (CPU) ho·∫∑c 10-20 ph√∫t (GPU)

### Option 3: Training tr√™n Google Colab
1. M·ªü notebook: `scripts/training/colab_notebook.ipynb`
2. Ch·∫°y c√°c cells theo th·ª© t·ª±
3. Model s·∫Ω t·ª± ƒë·ªông ƒë∆∞·ª£c t·∫£i v·ªÅ

## ‚öôÔ∏è T√πy Ch·ªânh Batch Size

### N·∫øu c√≥ GPU m·∫°nh:
S·ª≠a trong `config.yml`:
```yaml
- name: custom_components.phobert_featurizer.PhoBERTFeaturizer
  batch_size: 64  # TƒÉng l√™n 64-128
```

### N·∫øu ch·ªâ c√≥ CPU ho·∫∑c GPU y·∫øu:
S·ª≠a trong `config.yml`:
```yaml
- name: custom_components.phobert_featurizer.PhoBERTFeaturizer
  batch_size: 8  # Gi·∫£m xu·ªëng 8-16
```

### N·∫øu thi·∫øu RAM:
```yaml
- name: DIETClassifier
  batch_size: [4, 8]  # Gi·∫£m xu·ªëng
```

## üîß Troubleshooting

### Training v·∫´n ch·∫≠m?
1. **Ki·ªÉm tra GPU:**
   ```python
   import torch
   print(torch.cuda.is_available())  # Ph·∫£i l√† True
   ```

2. **Ki·ªÉm tra batch processing:**
   - Khi training, b·∫°n s·∫Ω th·∫•y log: `[PhoBERTFeaturizer] ‚úÖ GPU detected: ...`
   - N·∫øu kh√¥ng th·∫•y, GPU kh√¥ng ƒë∆∞·ª£c s·ª≠ d·ª•ng

3. **Gi·∫£m batch size:**
   - N·∫øu GPU memory ƒë·∫ßy, gi·∫£m `batch_size` trong config
   - N·∫øu CPU qu√° t·∫£i, gi·∫£m s·ªë workers

### Out of Memory?
1. Gi·∫£m `batch_size` trong `PhoBERTFeaturizer` (xu·ªëng 8-16)
2. Gi·∫£m `batch_size` trong `DIETClassifier` (xu·ªëng [4, 8])
3. Gi·∫£m `max_length` trong `PhoBERTFeaturizer` (xu·ªëng 128)

### GPU kh√¥ng ƒë∆∞·ª£c s·ª≠ d·ª•ng?
1. Ki·ªÉm tra CUDA ƒë√£ c√†i ƒë·∫∑t:
   ```bash
   nvidia-smi
   ```

2. C√†i ƒë·∫∑t PyTorch v·ªõi CUDA:
   ```bash
   pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
   ```

3. Ki·ªÉm tra PyTorch detect GPU:
   ```python
   import torch
   print(torch.cuda.is_available())
   ```

## üìù L∆∞u √ù

1. **Batch processing** ch·ªâ ho·∫°t ƒë·ªông khi c√≥ nhi·ªÅu messages c√πng l√∫c
2. **GPU** s·∫Ω t·ª± ƒë·ªông ƒë∆∞·ª£c s·ª≠ d·ª•ng n·∫øu c√≥
3. **Config local** gi·∫£m ch·∫•t l∆∞·ª£ng model m·ªôt ch√∫t nh∆∞ng train nhanh h∆°n nhi·ªÅu
4. **N√™n train tr√™n Colab** n·∫øu kh√¥ng c√≥ GPU m·∫°nh

## üéâ K·∫øt Qu·∫£

Sau c√°c t·ªëi ∆∞u h√≥a:
- ‚úÖ Training nhanh h∆°n 2-3x
- ‚úÖ S·ª≠ d·ª•ng GPU hi·ªáu qu·∫£ h∆°n
- ‚úÖ D·ªÖ d√†ng ki·ªÉm tra v√† debug
- ‚úÖ Config linh ho·∫°t cho m·ªçi m√¥i tr∆∞·ªùng

