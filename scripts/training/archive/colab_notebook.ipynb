{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üöÄ Train Rasa NLU v·ªõi PhoBERT-large tr√™n Google Colab\n",
        "\n",
        "Notebook n√†y s·∫Ω t·ª± ƒë·ªông:\n",
        "1. C√†i ƒë·∫∑t dependencies\n",
        "2. T·∫£i PhoBERT-large model\n",
        "3. Train NLU model\n",
        "4. Download model v·ªÅ m√°y local\n",
        "\n",
        "## ‚ö†Ô∏è L∆∞u √Ω Quan Tr·ªçng\n",
        "\n",
        "**T√™n th∆∞ m·ª•c sau khi clone l√† `ciesta-assistant` (kh√¥ng ph·∫£i `ciesta-asisstant`)**\n",
        "\n",
        "## üìã Y√™u c·∫ßu\n",
        "- Clone repository t·ª´ GitHub\n",
        "- Ho·∫∑c upload c√°c file c·∫ßn thi·∫øt v√†o Colab\n",
        "\n",
        "## üîß C√°ch s·ª≠ d·ª•ng\n",
        "1. Ch·∫°y cell ƒë·∫ßu ti√™n ƒë·ªÉ clone repo\n",
        "2. Ch·∫°y c√°c cell ti·∫øp theo ƒë·ªÉ train\n",
        "3. Ch·ªù training ho√†n t·∫•t (30 ph√∫t - 2 gi·ªù)\n",
        "4. Model s·∫Ω t·ª± ƒë·ªông ƒë∆∞·ª£c t·∫£i v·ªÅ m√°y local\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## B∆∞·ªõc 1: Upload files (N·∫øu ch∆∞a c√≥ trong Colab)\n",
        "\n",
        "N·∫øu b·∫°n ƒë√£ clone repo th√¨ b·ªè qua b∆∞·ªõc n√†y.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Upload files n·∫øu c·∫ßn\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## B∆∞·ªõc 2: Clone repository v√† Setup\n",
        "\n",
        "**QUAN TR·ªåNG:** T√™n th∆∞ m·ª•c l√† `ciesta-assistant` (kh√¥ng ph·∫£i `ciesta-asisstant`)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# SETUP T·ª∞ ƒê·ªòNG - CH·∫†Y CELL N√ÄY ƒê·∫¶U TI√äN\n",
        "# ============================================\n",
        "\n",
        "import os\n",
        "\n",
        "# B∆∞·ªõc 1: Clone repository\n",
        "if not os.path.exists(\"ciesta-assistant\"):\n",
        "    print(\"üì¶ ƒêang clone repository...\")\n",
        "    !git clone https://github.com/HoangPhucDE/ciesta-assistant.git\n",
        "    print(\"‚úÖ ƒê√£ clone repository th√†nh c√¥ng\")\n",
        "else:\n",
        "    print(\"‚úÖ Repository ƒë√£ t·ªìn t·∫°i\")\n",
        "\n",
        "# B∆∞·ªõc 2: Chuy·ªÉn v√†o th∆∞ m·ª•c\n",
        "%cd ciesta-assistant\n",
        "print(f\"‚úÖ ƒê√£ chuy·ªÉn v√†o: {os.getcwd()}\")\n",
        "\n",
        "# B∆∞·ªõc 3: Ki·ªÉm tra files c·∫ßn thi·∫øt\n",
        "print(\"\\nüìã Ki·ªÉm tra files c·∫ßn thi·∫øt:\")\n",
        "required_files = [\n",
        "    \"config.yml\",\n",
        "    \"data/nlu.yml\",\n",
        "    \"custom_components/phobert_featurizer.py\",\n",
        "    \"requirements.txt\"\n",
        "]\n",
        "\n",
        "missing = []\n",
        "for file in required_files:\n",
        "    if os.path.exists(file):\n",
        "        print(f\"  ‚úÖ {file}\")\n",
        "    else:\n",
        "        print(f\"  ‚ùå {file} - KH√îNG T√åM TH·∫§Y\")\n",
        "        missing.append(file)\n",
        "\n",
        "if missing:\n",
        "    print(f\"\\n‚ö† C·∫£nh b√°o: Thi·∫øu c√°c file: {', '.join(missing)}\")\n",
        "else:\n",
        "    print(\"\\n‚úÖ T·∫•t c·∫£ files c·∫ßn thi·∫øt ƒë√£ c√≥ s·∫µn!\")\n",
        "    print(\"\\nüöÄ S·∫µn s√†ng ƒë·ªÉ train! Ch·∫°y cell ti·∫øp theo ƒë·ªÉ b·∫Øt ƒë·∫ßu.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## B∆∞·ªõc 3: Ch·∫°y script training t·ª± ƒë·ªông\n",
        "\n",
        "Script s·∫Ω t·ª± ƒë·ªông:\n",
        "- C√†i ƒë·∫∑t dependencies\n",
        "- T·∫£i PhoBERT-large\n",
        "- Train NLU model\n",
        "- Download model v·ªÅ m√°y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# TRAIN NLU MODEL\n",
        "# ============================================\n",
        "\n",
        "import os\n",
        "print(f\"üìÇ Th∆∞ m·ª•c hi·ªán t·∫°i: {os.getcwd()}\")\n",
        "\n",
        "# Train NLU (s·ª≠ d·ª•ng Python 3.10 t·ª´ venv)\n",
        "print(\"‚è≥ B·∫Øt ƒë·∫ßu training...\")\n",
        "print(\"üí° Qu√° tr√¨nh n√†y c√≥ th·ªÉ m·∫•t 30 ph√∫t - 2 gi·ªù\")\n",
        "print(\"üí° Model s·∫Ω ƒë∆∞·ª£c t·ª± ƒë·ªông t·∫£i t·ª´ HuggingFace khi c·∫ßn\\n\")\n",
        "\n",
        "!venv_py310/bin/python -m rasa train nlu\n",
        "\n",
        "print(\"\\n‚úÖ Training ho√†n t·∫•t!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## B∆∞·ªõc 4: Training th·ªß c√¥ng (N·∫øu c·∫ßn)\n",
        "\n",
        "N·∫øu script t·ª± ƒë·ªông kh√¥ng ho·∫°t ƒë·ªông, b·∫°n c√≥ th·ªÉ ch·∫°y t·ª´ng b∆∞·ªõc th·ªß c√¥ng:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# C√†i ƒë·∫∑t dependencies\n",
        "%pip install -q -r requirements.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# T·∫£i PhoBERT-large model\n",
        "from huggingface_hub import snapshot_download\n",
        "import os\n",
        "\n",
        "os.makedirs(\"models_hub/phobert-large\", exist_ok=True)\n",
        "\n",
        "snapshot_download(\n",
        "    repo_id=\"vinai/phobert-large\",\n",
        "    local_dir=\"models_hub/phobert-large\",\n",
        "    local_dir_use_symlinks=False,\n",
        "    resume_download=True\n",
        ")\n",
        "\n",
        "print(\"‚úÖ ƒê√£ t·∫£i model th√†nh c√¥ng\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# T·∫°o symlink ho·∫∑c copy model\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "os.makedirs(\"models\", exist_ok=True)\n",
        "\n",
        "if os.path.exists(\"models/phobert-large\"):\n",
        "    if os.path.islink(\"models/phobert-large\"):\n",
        "        os.unlink(\"models/phobert-large\")\n",
        "    else:\n",
        "        shutil.rmtree(\"models/phobert-large\")\n",
        "\n",
        "try:\n",
        "    os.symlink(\"../models_hub/phobert-large\", \"models/phobert-large\")\n",
        "    print(\"‚úÖ ƒê√£ t·∫°o symlink\")\n",
        "except:\n",
        "    shutil.copytree(\"models_hub/phobert-large\", \"models/phobert-large\")\n",
        "    print(\"‚úÖ ƒê√£ copy model\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train NLU model\n",
        "!rasa train nlu\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# T·∫¢I MODEL V·ªÄ M√ÅY LOCAL\n",
        "# ============================================\n",
        "\n",
        "from google.colab import files\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "# ƒê·∫£m b·∫£o ƒëang ·ªü ƒë√∫ng th∆∞ m·ª•c\n",
        "if not os.path.exists(\"models\"):\n",
        "    print(\"‚ö† Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c models\")\n",
        "    print(\"üí° ƒê·∫£m b·∫£o ƒë√£ ch·∫°y cell training tr∆∞·ªõc\")\n",
        "else:\n",
        "    models_dir = Path(\"models\")\n",
        "    model_files = list(models_dir.glob(\"*.tar.gz\"))\n",
        "    \n",
        "    if model_files:\n",
        "        # Get latest model\n",
        "        latest_model = max(model_files, key=lambda x: x.stat().st_mtime)\n",
        "        size_mb = latest_model.stat().st_size / (1024*1024)\n",
        "        \n",
        "        print(f\"üì¶ Model: {latest_model.name}\")\n",
        "        print(f\"üìä K√≠ch th∆∞·ªõc: {size_mb:.2f} MB\")\n",
        "        print(f\"üìÅ ƒê∆∞·ªùng d·∫´n: {latest_model}\")\n",
        "        print(\"\\n‚¨áÔ∏è ƒêang t·∫£i v·ªÅ m√°y...\")\n",
        "        \n",
        "        files.download(str(latest_model))\n",
        "        print(\"‚úÖ ƒê√£ b·∫Øt ƒë·∫ßu t·∫£i model v·ªÅ m√°y local\")\n",
        "    else:\n",
        "        print(\"‚ùå Kh√¥ng t√¨m th·∫•y model\")\n",
        "        print(\"üí° Vui l√≤ng ki·ªÉm tra l·∫°i qu√° tr√¨nh training\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîç Troubleshooting\n",
        "\n",
        "### L·ªói: Out of Memory\n",
        "- Gi·∫£m `batch_size` trong `config.yml`\n",
        "  ```yaml\n",
        "  batch_size: [8, 16]  # Thay v√¨ [16, 32]\n",
        "  ```\n",
        "\n",
        "### L·ªói: Training qu√° l√¢u tr√™n Local\n",
        "**‚ö†Ô∏è QUAN TR·ªåNG:** N·∫øu training tr√™n local machine r·∫•t ch·∫≠m:\n",
        "\n",
        "1. **Ki·ªÉm tra GPU:**\n",
        "   ```bash\n",
        "   python scripts/training/check_training_setup.py\n",
        "   ```\n",
        "\n",
        "2. **S·ª≠ d·ª•ng config t·ªëi ∆∞u cho local:**\n",
        "   ```bash\n",
        "   rasa train nlu --config config_local.yml\n",
        "   ```\n",
        "   \n",
        "3. **C√°c t·ªëi ∆∞u h√≥a ƒë√£ ƒë∆∞·ª£c th√™m:**\n",
        "   - ‚úÖ Batch processing trong PhoBERTFeaturizer (nhanh h∆°n 10-50x)\n",
        "   - ‚úÖ T·ª± ƒë·ªông detect v√† s·ª≠ d·ª•ng GPU\n",
        "   - ‚úÖ Config t·ªëi ∆∞u v·ªõi √≠t epochs h∆°n\n",
        "\n",
        "4. **N·∫øu kh√¥ng c√≥ GPU:**\n",
        "   - Gi·∫£m `epochs` trong `config.yml`:\n",
        "     ```yaml\n",
        "     epochs: 300  # Thay v√¨ 600\n",
        "     ```\n",
        "   - Gi·∫£m `batch_size`:\n",
        "     ```yaml\n",
        "     batch_size: [8, 16]  # Thay v√¨ [16, 32]\n",
        "     ```\n",
        "   - Gi·∫£m PhoBERT batch_size:\n",
        "     ```yaml\n",
        "     batch_size: 16  # Trong PhoBERTFeaturizer\n",
        "     ```\n",
        "   - **Khuy·∫øn ngh·ªã:** Train tr√™n Google Colab (c√≥ GPU mi·ªÖn ph√≠)\n",
        "\n",
        "### L·ªói: Kh√¥ng t√¨m th·∫•y model\n",
        "- Ki·ªÉm tra ƒë∆∞·ªùng d·∫´n trong `config.yml`:\n",
        "  ```yaml\n",
        "  model_name: \"models/phobert-large\"\n",
        "  ```\n",
        "\n",
        "### L·ªói: Import error\n",
        "- Ch·∫°y l·∫°i cell c√†i ƒë·∫∑t dependencies\n",
        "- Ki·ªÉm tra `requirements.txt` c√≥ ƒë·∫ßy ƒë·ªß kh√¥ng\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
