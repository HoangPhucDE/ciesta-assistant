# ============================================
# T·ªêI ∆ØU GPU - CH·∫†Y TRONG COLAB
# ============================================
# Script n√†y ki·ªÉm tra v√† t·ªëi ∆∞u batch size ƒë·ªÉ t·∫≠n d·ª•ng GPU t·ªët h∆°n
# Ch·∫°y script n√†y TR∆Ø·ªöC KHI train ho·∫∑c D·ª™NG training hi·ªán t·∫°i v√† ch·∫°y l·∫°i

import os
import re
import subprocess
from pathlib import Path

print("‚ö° T·ªëi ∆∞u GPU Utilization cho Rasa Training")
print("=" * 60)

# Ki·ªÉm tra GPU
print("\nüéÆ Ki·ªÉm tra GPU...")
try:
    result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)
    if result.returncode == 0:
        print("‚úÖ GPU ƒë∆∞·ª£c ph√°t hi·ªán")
        # Extract GPU name
        if "T4" in result.stdout:
            print("   GPU: T4 (15GB)")
        elif "V100" in result.stdout:
            print("   GPU: V100 (16GB)")
        elif "A100" in result.stdout:
            print("   GPU: A100 (40GB)")
        else:
            print("   GPU: Unknown")
    else:
        print("‚ùå Kh√¥ng t√¨m th·∫•y GPU")
        print("   Vui l√≤ng b·∫≠t GPU: Runtime ‚Üí Change runtime type ‚Üí GPU")
        exit(1)
except Exception as e:
    print(f"‚ö†Ô∏è L·ªói khi ki·ªÉm tra GPU: {e}")

# Ki·ªÉm tra GPU memory b·∫±ng PyTorch
print("\nüîç Ki·ªÉm tra GPU Memory...")
check_gpu_memory = """
try:
    import torch
    if torch.cuda.is_available():
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
        gpu_name = torch.cuda.get_device_name(0)
        print(f"{gpu_memory:.1f}|{gpu_name}")
    else:
        print("0|No GPU")
except Exception as e:
    print(f"0|Error: {e}")
"""
with open("/tmp/check_gpu_memory.py", "w") as f:
    f.write(check_gpu_memory)

try:
    result = subprocess.run(
        ["python3", "/tmp/check_gpu_memory.py"],
        capture_output=True,
        text=True,
        cwd=os.getcwd()
    )
    if result.returncode == 0 and "|" in result.stdout:
        parts = result.stdout.strip().split("|")
        gpu_memory_gb = float(parts[0])
        gpu_name = parts[1] if len(parts) > 1 else "Unknown"
        print(f"   GPU: {gpu_name}")
        print(f"   Memory: {gpu_memory_gb:.1f} GB")
    else:
        print("   ‚ö†Ô∏è Kh√¥ng th·ªÉ ki·ªÉm tra GPU memory")
        gpu_memory_gb = None
except Exception as e:
    print(f"   ‚ö†Ô∏è L·ªói: {e}")
    gpu_memory_gb = None

# T√¨m config file
print("\nüìÅ T√¨m config file...")
current_dir = Path.cwd()
config_paths = [
    current_dir / "config.yml",
    current_dir / "config/rasa/config.yml",
]

config_file = None
for path in config_paths:
    if path.exists():
        config_file = path
        print(f"   ‚úÖ T√¨m th·∫•y: {config_file}")
        break

if not config_file:
    print("   ‚ùå Kh√¥ng t√¨m th·∫•y config.yml")
    print("   Vui l√≤ng ƒë·∫£m b·∫£o ƒëang ·ªü ƒë√∫ng th∆∞ m·ª•c project")
    exit(1)

# ƒê·ªçc config hi·ªán t·∫°i
print("\nüìñ ƒê·ªçc config hi·ªán t·∫°i...")
with open(config_file, "r", encoding="utf-8") as f:
    config_content = f.read()

# Ki·ªÉm tra batch size hi·ªán t·∫°i
phobert_batch_match = re.search(r'pooling_strategy:\s*"mean_max"\s*\n\s*batch_size:\s*(\d+)', config_content)
diet_batch_match = re.search(r'batch_size:\s*\[(\d+),\s*(\d+)\]', config_content)

if phobert_batch_match:
    phobert_batch = int(phobert_batch_match.group(1))
    print(f"   PhoBERTFeaturizer batch_size: {phobert_batch}")
else:
    print("   ‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y PhoBERTFeaturizer batch_size")
    phobert_batch = None

if diet_batch_match:
    diet_batch = [int(diet_batch_match.group(1)), int(diet_batch_match.group(2))]
    print(f"   DIETClassifier batch_size: {diet_batch_match.group(0)}")
else:
    print("   ‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y DIETClassifier batch_size")
    diet_batch = None

# ƒê·ªÅ xu·∫•t batch size t·ªëi ∆∞u
print("\nüí° ƒê·ªÅ xu·∫•t batch size t·ªëi ∆∞u:")
if gpu_memory_gb and gpu_memory_gb >= 15:
    recommended_phobert = 128
    recommended_diet = [128, 256]
    print(f"   GPU l·ªõn ({gpu_memory_gb:.1f}GB) - Khuy·∫øn ngh·ªã:")
    print(f"   - PhoBERTFeaturizer: {recommended_phobert}")
    print(f"   - DIETClassifier: {recommended_diet}")
elif gpu_memory_gb and gpu_memory_gb >= 8:
    recommended_phobert = 64
    recommended_diet = [64, 128]
    print(f"   GPU trung b√¨nh ({gpu_memory_gb:.1f}GB) - Khuy·∫øn ngh·ªã:")
    print(f"   - PhoBERTFeaturizer: {recommended_phobert}")
    print(f"   - DIETClassifier: {recommended_diet}")
elif gpu_memory_gb and gpu_memory_gb >= 4:
    recommended_phobert = 48
    recommended_diet = [32, 64]
    print(f"   GPU nh·ªè ({gpu_memory_gb:.1f}GB) - Khuy·∫øn ngh·ªã:")
    print(f"   - PhoBERTFeaturizer: {recommended_phobert}")
    print(f"   - DIETClassifier: {recommended_diet}")
else:
    print("   ‚ö†Ô∏è Kh√¥ng th·ªÉ detect GPU memory - Gi·ªØ batch size hi·ªán t·∫°i")
    recommended_phobert = None
    recommended_diet = None

# Ki·ªÉm tra xem c√≥ c·∫ßn update kh√¥ng
need_update = False
if recommended_phobert and phobert_batch and phobert_batch < recommended_phobert:
    print(f"\n‚ö†Ô∏è PhoBERT batch_size ({phobert_batch}) nh·ªè h∆°n khuy·∫øn ngh·ªã ({recommended_phobert})")
    need_update = True

if recommended_diet and diet_batch:
    if diet_batch[0] < recommended_diet[0] or diet_batch[1] < recommended_diet[1]:
        print(f"‚ö†Ô∏è DIET batch_size ({diet_batch}) nh·ªè h∆°n khuy·∫øn ngh·ªã ({recommended_diet})")
        need_update = True

# Update config n·∫øu c·∫ßn
if need_update and recommended_phobert and recommended_diet:
    print("\nüîÑ C·∫≠p nh·∫≠t config...")
    original_content = config_content
    
    # Update PhoBERT batch_size
    if phobert_batch and phobert_batch < recommended_phobert:
        config_content = re.sub(
            r'(pooling_strategy:\s*"mean_max"\s*\n\s*batch_size:)\s*\d+(\s*#.*)?',
            f'\\1 {recommended_phobert}  # T·ªëi ∆∞u cho GPU ({gpu_memory_gb:.1f}GB)',
            config_content
        )
        print(f"   ‚úÖ ƒê√£ c·∫≠p nh·∫≠t PhoBERT batch_size: {phobert_batch} ‚Üí {recommended_phobert}")
    
    # Update DIET batch_size
    if diet_batch:
        config_content = re.sub(
            r'(batch_size:\s*)\[\d+,\s*\d+\]',
            f'\\1{recommended_diet}  # T·ªëi ∆∞u cho GPU ({gpu_memory_gb:.1f}GB)',
            config_content
        )
        print(f"   ‚úÖ ƒê√£ c·∫≠p nh·∫≠t DIET batch_size: {diet_batch} ‚Üí {recommended_diet}")
    
    # Ghi l·∫°i config
    if config_content != original_content:
        with open(config_file, "w", encoding="utf-8") as f:
            f.write(config_content)
        print("\n‚úÖ ƒê√£ c·∫≠p nh·∫≠t config.yml")
        print("\n‚ö†Ô∏è QUAN TR·ªåNG:")
        print("   1. D·ª™NG training hi·ªán t·∫°i (n·∫øu ƒëang ch·∫°y)")
        print("   2. Ch·∫°y l·∫°i training v·ªõi config m·ªõi:")
        print("      !rasa train nlu --config config.yml")
        print("   3. GPU RAM usage s·∫Ω tƒÉng l√™n 50-70%")
        print("   4. Training s·∫Ω nhanh h∆°n ƒë√°ng k·ªÉ")
    else:
        print("   ‚ÑπÔ∏è Config ƒë√£ ƒë∆∞·ª£c t·ªëi ∆∞u")
else:
    if not need_update:
        print("\n‚úÖ Batch size ƒë√£ ƒë∆∞·ª£c t·ªëi ∆∞u!")
        print("   GPU s·∫Ω ƒë∆∞·ª£c s·ª≠ d·ª•ng hi·ªáu qu·∫£ h∆°n")
    else:
        print("\n‚ö†Ô∏è Kh√¥ng th·ªÉ t·ª± ƒë·ªông t·ªëi ∆∞u (kh√¥ng detect ƒë∆∞·ª£c GPU)")
        print("   C√≥ th·ªÉ tƒÉng batch size th·ªß c√¥ng trong config.yml")

print("\n" + "=" * 60)
print("üí° Tip: Monitor GPU usage trong Resources panel")
print("   GPU RAM n√™n s·ª≠ d·ª•ng 50-70% khi training v·ªõi batch size t·ªëi ∆∞u")

